---
output:
  md_document:
    variant: html
---

# Purpose

The purpose of this work folder is to provide a sanitized workflow for the code and data wrangling underlying the Financial Econometrics 871 practical exam before compiling the final reports in pdf formats as given above.

To get started..


```{r, echo=FALSE}

rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
library(tidyverse)
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))

# Installing and/or loading packages

pacman::p_load("tidyverse","fmxdat", "dplyr", "rmsfuns", "PerformanceAnalytics", "tbl2xts", "xts", "rportfolios", "Texevier", "kableExtra", "ggplot2", "extrafont", "devtools")

```


# Question 1

The purpose of this question is to prepare a presentation where I showcast the performance of my fund (called Our Fund
), specifically putting my fundâ€™s performance into perspective by comparing it to the benchmark (Capped SWIX) as well as to industry peers (ASISA active managers).

## Loading data

```{r, Loading Data}

ASISA <- read_rds("data/ASISA.rds") # ASISA Active Managers. Notice that there are 227 different actively managed funds.

                                    # Monthly observations, 2002-11-30 to 2022-10-31

BM <- read_rds("data/Capped_SWIX.rds") # Benchmark: Capped Swix. Monthly observations, 1999-12-31 to 2022-10-31

AI_Fund <- read_rds("data/AI_Max_Fund.rds") # My Systematic AI Fund. Monthly observations, 2003-01-31 to 2022-10-31

```

## Rolling 3 Year Annualized Returns Comparisson

To plot the Rolling 3 Year Annualized Returns, I start by merging the three datasets, calculate the Rolling 3 Year Annualized Returns, determine the 90, 75, 50, 25, and 10 percent percentiles. Thereafter, I graph the rolling returns of Our Fund versus the benchmark Capped SWIX (J433) versus the ASISA Percentiles.

```{r, Rolling 3 Year Annualized Returns}

pacman::p_load(RcppRoll)


# I start by merging the three datasets

df <- left_join(AI_Fund |> pivot_wider(names_from = "Tickers", values_from = "Returns"), BM |> pivot_wider(names_from = "Tickers", values_from = "Returns") ,by ="date") |> left_join(ASISA |> pivot_wider(names_from = "Name", values_from = "Returns"), by="date") |> 
    
    pivot_longer(cols = -date, names_to = "Name", values_to = "Returns")


# I calculate the rolling returns on a 3 year annualized basis

df_rolling <- df |> arrange(date) |> group_by(Name) |> 
    mutate(Roll_rets = RcppRoll::roll_prod(1+ Returns, 36, fill = NA, align = "right")^(12/36) -1) |> 
    group_by(date) |> 
    filter(any(!is.na(Roll_rets))) |> # Remove dates with no rolling returns
    ungroup()

# Next, I determine the top decile, top quartile, median, bottom decile and bottom quartile for the ASISA funds for each month

ASISA_percentiles <- df_rolling |> filter(!(Name %in% c("Our_Fund", "J433")) ) |> select(-Returns) |> 
    
    group_by(date) |> mutate("Top decile" = quantile(Roll_rets ,probs = 0.9, na.rm=T)) |>  
    
    group_by(date) |> mutate("Top quartile" = quantile(Roll_rets ,probs = 0.75, na.rm=T)) |>  
    
    group_by(date) |> mutate("Median" = quantile(Roll_rets ,probs = 0.5, na.rm=T)) |>  
    
    group_by(date) |> mutate("Bottom quartile" = quantile(Roll_rets ,probs = 0.25, na.rm=T)) |>  
    
    group_by(date) |> mutate("Bottom decile" = quantile(Roll_rets ,probs = 0.1, na.rm=T))  |> 
    
    select(- c(Name, Roll_rets)) |> ungroup() |> 
    
    unique() 

# And merge it with Our Fund and Benchmark Rolling 3 year annualized returns

Comparisson_df_plot <-

df_rolling |> filter((Name %in% c("Our_Fund", "J433")) ) |> select(-Returns) |>  
    
    pivot_wider(names_from = "Name", values_from = "Roll_rets") |> 
    
    left_join(ASISA_percentiles, by = "date") |> 
    
    pivot_longer(cols= -date, names_to = "Name", values_to = "RollRet")

# Now I graph the rolling returns of our fund versus the benchmark Capped SWIX (J433) vesus the ASISA Percentiles

Comparisson_plot <- Comparisson_df_plot |>   
    
    ggplot() + 
  
  geom_line(aes(date, RollRet, color = Name), size = 0.9, alpha = 0.7) + 
  
  fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(25),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
  
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "%", caption = "Note:\nCalculation own",
       title = "Benchmark (J433) vs Our Fund vs ASISA Active Funds: Rolling 3 Year Annualized Returns",
       subtitle = "The ASISA Active Funds are reflected by the monthly rollling 3 year annualized percentiles.")
  
# Finplot for finishing touches easily:

  fmxdat::finplot(Comparisson_plot, x.vert = T, x.date.type = "%Y", x.date.dist = "2 years", )

```

## Excess Returns and Tracking Error Comparison

Next I annualize the returns on a 15 year basis, and plot the Excess Returns over the benchmark (Capped SWIX) to the Tracking Error over the past 90 months. 

```{r, Tracking Errors}

funds_xts <- df_rolling |> filter(!(Name %in% c("J433"))) |> 
    
    filter(date >= lubridate::ymd("2007-10-31") ) |> # Filter dates to calc annualized, 15 year basis, returns below
    
    select(-Roll_rets) |> tbl_xts(cols_to_xts = Returns, spread_by = Name)
    
Benchmark_xts <- df_rolling |> filter(Name %in% c("J433")) |> 
    
    filter(date >= lubridate::ymd("2007-10-31") ) |>  # Filter dates to calc annualized, 15 year basis, returns below
    
    select(-Roll_rets) |> 
    
    tbl_xts()    



 
# Now I attempt to annualize the returns on a 15 year basis

funds_ann <- PerformanceAnalytics::Return.annualized(funds_xts, scale = 12) |> as_tibble() |> 
    
    pivot_longer(cols = 1:228, names_to = "Name", values_to = "Ret")

Benchmark_ann <- PerformanceAnalytics::Return.annualized(Benchmark_xts, scale = 12)|> as_tibble() # = 0.0801 for future use

# Now to get the tracking errors

Tracking_errors <- PerformanceAnalytics::TrackingError(funds_xts, Benchmark_xts, scale = 12) |> as_tibble() |> 
    
    pivot_longer(cols = 1:228, names_to = "Name", values_to = "TE")

Excess_returns_df <- inner_join(funds_ann, Tracking_errors, by= "Name") |> 
    
    mutate(Excess_ret = Ret - 0.0801) |> 
    
    mutate(Median_Ret = quantile(Ret, probs=0.5, na.rm = T)) # And add the median return for plot





# Lastly, I plot the results:

Excess_returns_plot <- Excess_returns_df |> 
    
    ggplot() +
    
    geom_point(aes(TE, Excess_ret), size = 2, alpha= 0.7) + 
    
             # Noting that Our_Fund is at the coordinate: (0.042,0.0281)
    
     geom_point(aes(x = 0.042, y = 0.0281), size = 8, color ="red" ) +
    
    geom_text(aes(x = 0.042, y = 0.0281), label="Our Fund", vjust=0.5, color = "blue") + # this adds a label for the red point
    
    geom_vline(xintercept=0.0640, linetype="dotted") +
    
    geom_text(aes(x = 0.067, y = -0.22), label="Median", vjust=0) +
    
     fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(25),
                    caption.size = fmxdat::ggpts(25),  
                    CustomCaption = T) + 
    

  
  fmxdat::fmx_cols() + 
  
  labs(x = "Tracking Error to Capped SWIX", y = "Ann. Return Differential vs Capped SWIX", caption = "Note:\nCalculation own. Returns are annualized, on a 90 month (15 year) basis.",
       title = "Excess Returns and Tracking Error Comparison",
       subtitle = "Benchmark: Capped SWIX. Tracking Error over the past 90 months")
  
# Finplot for finishing touches easily:

  fmxdat::finplot(Excess_returns_plot, x.vert = F, x.pct = T, y.pct = T )




```


# Question 2

Economists recently pointed out that the current yield spreads in local mid to longer dated
bond yields have since 2020 been the highest in decades.

Using the data/SA Bonds.rds file, I conduct an analysis of the current yield spreads in the local
bond market. In addition, I compare the local spread to comparable international spreads, observe the correlation between the local bond spreads and the USD-ZAR level, as well as consider the SA 10 Year Break-Even inflation estimate.

## Loading data

```{r, Data Import}

SA_bonds <- read_rds("data/SA_Bonds.rds")
BE_Infl <- read_rds("data/BE_Infl.rds")
bonds_2y <- read_rds("data/bonds_2y.rds")
bonds_10y <- read_rds("data/bonds_10y.rds")
usdzar <- read_rds("data/usdzar.rds")
ZA_Infl <- read_rds("data/ZA_Infl.rds")
IV <- read_rds("data/IV.rds")

```

## Global Bond Market Yields

Conduct data wrangling to get the 2Yr and 10Yr global yields and their spreads in one tbl..

```{r, Global Bond Yield Spreads Data Wrangling}

Countries_to_compare <- c("Germany", "ZA", "US", "CHINA", "Japan", "Brazil")

bonds_10y_adj <- bonds_10y |> pivot_wider(names_from = "Name", values_from = "Bond_10Yr") |> 
    
    left_join(SA_bonds |> select(c(date, ZA_10Yr)), by="date") |> 
    
    pivot_longer(cols = -date, names_to = "Name", values_to = "Bond_10Yr")|> 
    
    mutate(Name = gsub("_10Yr", "", Name)) |> 
    
    filter((Name %in% Countries_to_compare))  # Only filter the desired countries

# Upon initial attempt to format bonds_2y, I realised there is a duplicate entry, identified by the following 

duplicate_entries_identification <- bonds_2y %>%
                                    dplyr::group_by(date, Name) %>%
                                    dplyr::summarise(n = dplyr::n(), .groups = "drop") %>%
                                    dplyr::filter(n > 1L)

# Since the duplicate entry is "Chile_2yr", I remove Chile in order to pivot_wider. 

bonds_2y_adj <- bonds_2y |> filter(!(Name %in% c("Chile_2yr"))) |>  # remove chile
    
    pivot_wider(names_from = "Name", values_from = "Bond_2Yr") |> 
    
    left_join(SA_bonds |> select(c(date, ZA_2Yr)), by="date") |> 
    
    pivot_longer(cols = -date, names_to = "Name", values_to = "Bond_2Yr") |> 
    
    mutate(Name = gsub("_2yr", "", Name)) |> mutate(Name = gsub("_2Yr", "", Name)) |> 
    
    filter(Name %in% Countries_to_compare) 

# Now I proceed to merge the 2Yr and 10Yr global yields and their spreads in one tbl

Global_bonds_data <- inner_join(bonds_2y_adj, bonds_10y_adj, by= c("date", "Name")) |> 
    
    mutate(Spread = Bond_10Yr - Bond_2Yr)

```

And finally the plot..

```{r, Global Bond Yield Spreads Plot}

Global_bonds_plot <-    Global_bonds_data |> select(date,Name ,Spread) |> 
    
    ggplot() + 
  
  geom_line(aes(date, Spread , color = Name), size = 0.8, alpha = 0.7) +
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "Yield Spread (%)", caption = "Note:\nCalculation own",
       title = "Global Bond Market Yield Spreads",
       subtitle = "")
  
# Finplot for finishing touches:

fmxdat::finplot(Global_bonds_plot, x.vert = T, x.date.type = "%Y", x.date.dist = "2 years", darkcol = F)




```

In analyzing the above plot, the SA Bond yield spread has been significantly more volatile than those of Germany, China, the US, and Japan, while having less volatility in the last few years than Brazil. Visually confirming the notion that the current yield spreads in local mid to longer dated bond yields have since 2020 been the highest in decades. 


## SA Bond Yields, Spread, and ZAR/USD Exchange Rate

I now plot the SA bond yield spreads together with the ZAR/USD exchange rate. It is evident that  the comovements between the ZAR/USD and the SA bonds yield spread was rather strong, however, between the periods of 2010 and 2016 this positive correlation has diminished, likely due to the large open market asset purchases by the FED that distorts the international spillovers. 

```{r, US and SA Real Yield Spread}

SA_Bonds_Plot <- Global_bonds_data |> filter(Name %in% c("ZA")) |> left_join(usdzar |> select(-Name), by = "date") |> 
    
    rename( R_USD = Price) |> pivot_longer(cols = -c(date, Name), names_to = "Description", values_to = "Values") |> 
    
    ggplot() + 
  
  geom_line(aes(date, Values , color = Description), size = 0.8, alpha = 0.7) +
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "", caption = "Note:\nCalculation own",
       title = "SA Bond Yields, Spread, and ZAR/USD Exchange Rate",
       subtitle = "")
    
# Finplot for finishing touches:

fmxdat::finplot(SA_Bonds_Plot, x.vert = T, x.date.type = "%Y", x.date.dist = "2 years", darkcol = F)

```

## Statistics comparison pre and post GFC

Now I compare the SA vs US Bond Yields Spread statistics pre GFC vs post GFC to see whether it has changed significantly. Overall, considering the SA and US separately and rather loosely, the volatility in the SA bond yield has remained rather stable comparing it pre and post GFC.

```{r, Statistical Tables Pre and Post GFC US and SA}

# Partition ZA and US yield spread data into post and pre GFC and convert to xts

pre_GFC_xts <- Global_bonds_data|> filter(Name %in% c("ZA", "US")) |> select(date, Name, Spread) |> 
     
    filter(date <= lubridate::ymd(20081031)) |>  
    
    filter(date >= lubridate::ymd(19991206)) |>  # Start from ZA's first observation
    
    tbl_xts(cols_to_xts = Spread, spread_by = Name) 
    
post_GFC_xts <- Global_bonds_data|> filter(Name %in% c("ZA", "US")) |> select(date,Name ,Spread) |> 
     
    filter(date >= lubridate::ymd(20081031)) |> tbl_xts(cols_to_xts = Spread, spread_by = Name) 

# Use performance analytics package for statistical table

table_pre_GFC <- PerformanceAnalytics::table.Stats(pre_GFC_xts, ci=0.95, digits = 2)

table_post_GFC <- PerformanceAnalytics::table.Stats(post_GFC_xts, ci=0.95, digits = 2)

# Only select the desired stats

table_pre_GFC <- table_pre_GFC[c(3,6,9,10,11,12,14,15,16),] 
table_post_GFC <- table_post_GFC[c(3,6,9,10,11,12,14,15,16),] 

# Finally summarise neatly in a table using kable

final_stats_table <- 
    
    table_pre_GFC |>  data.frame() |>  tibble::rownames_to_column()|> 
    left_join(table_post_GFC|>  data.frame() |>  tibble::rownames_to_column(), by = "rowname" ) |> 
    rename(Description = rowname) |> 
    
    knitr::kable(col.names = c("Description",
                           "SA",
                           "US",
                           "SA", "US")) |> kable_classic(full_width = F) |> 
    
    add_header_above(c(" " = 1, "Pre GFC" = 2, "Post GFC" = 2))



final_stats_table

```

## SA Break-Even inflation yield estimate

Next, I analyse the SA Break-Even inflation yield estimate and compare it to the SA inflation rate. Break-even inflation is the difference between the nominal yield on a fixed-rate investment and the real yield (fixed spread) on an inflation-linked investment of similar maturity and credit quality. If inflation averages more than the break-even, the inflation-linked investment will outperform the fixed-rate.

```{r, SA Break-Even Inflation Yield}

# Break-even inflation is the difference between the nominal yield on a fixed-rate investment and the real yield (fixed spread) on an inflation-linked investment of similar maturity and credit quality. If inflation averages more than the break-even, the inflation-linked investment will outperform the fixed-rate.

# Find the monthly values of BE Infl Yiels to compare to monthly inflation data 

BE_Infl_adj <- BE_Infl |> mutate(YearMonth = format(date, "%Y-%m")) |> 
    
    group_by(YearMonth) |> filter(date == last(date)) |> 
    
    ungroup() |> rename(BEI = Price)|> select(YearMonth, BEI)

# Notice that the number of days withing the BE inflation set is incorrect, so I rather use YearMonth as common column, 
# And then utilise the dateconverter command. 

BEI_infl_plot <- ZA_Infl |> mutate(YearMonth = format(date, "%Y-%m") ) |> 
    
    select(YearMonth, Price) |> rename(Inflation = Price) |> 
    
    right_join(BE_Infl_adj, by = "YearMonth") |> 
    
    mutate(date = dateconverter(as.Date("2012-05-01"), as.Date("2021-10-29"), "calendarEOM")) |> 
    
    select(-YearMonth) |> 
    
    pivot_longer(cols = -date, names_to = "Name", values_to = "Values") |> 
    
     ggplot() + 
  
  geom_line(aes(date, Values , color = Name), size = 0.8, alpha = 0.7) +
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "%", caption = "Note:\nCalculation own",
       title = "SA Break-Even Inflation Yield Versus Average Inflation Rate",
       subtitle = "")

# Finplot for finishing touches:

fmxdat::finplot(BEI_infl_plot, x.vert = T, x.date.type = "%Y", x.date.dist = "2 years", darkcol = F)

```

From the plot above, the SA average inflation rate has not surpassed the BE inflation yield estimate since 2014, indicating that fixed investment has been firmly preferred over the index-linked bond investment. 


# Question 3 

The purpose of this question is to, using the information on the ALSI (J200) and SWIX (J400) top 40 Indexes, compare the SWIX and ALSI methodologies by looking at the performance of different sector exposures and stock concentrations over
time.

## Data import

```{r, Data import}

T40 <- read_rds("data/T40.rds") # There are 92 stocks in this tbl

RebDays <- read_rds("data/Rebalance_days.rds")

Capped_SWIX <- read_rds("data/Capped_SWIX.rds") # This is the Monthly Capped and Weighted Portf Returns for SWIX Index (J433)

# I first shrink the dataframe to include only what in needed

T40_a <- T40 |> select(-Short.Name) |> 
    
    mutate(Tickers = gsub(" SJ Equity", "", Tickers))  # Remove clutter in Tickers names

```

## ALSI and SWIX weighted portfolio cumulative returns

I first plot the ALSI and SWIX weighted portfolio cumulative returns, following some tedious data wrangling. 


```{r, ALSI and SWIX wieghted port returns by hand}

# I generate a tbl calculating both Indexes weighted returns by hand

df_Port_ret <- T40_a |> 
    
    mutate(J400 = coalesce(J400, 0)) |> 
    
    mutate(J400 = coalesce(J400, 0)) |> 
    
    mutate(ALSI_wret = Return*J200) |> 
    
    mutate(SWIX_wret = Return*J400) |> 
    
    arrange(date) |> 
    
    group_by(date) |> 
    
    mutate(ALSI_pret = sum(ALSI_wret)) |> 
    
    mutate(SWIX_pret = sum(SWIX_wret)) 
    
```

```{r, ALSI and SWIX weighted portfoio returns using Safe_returns}


# Lets calculate the weighted portfolio daily return for ALSI and SWIX using Safe_Returns to verify my by hand calculation

Wghts_ALSI_xts <- T40_a |> select(date, Tickers , J200) |> tbl_xts(cols_to_xts = J200, spread_by = Tickers)

Wghts_SWIX_xts <- T40_a |> select(date, Tickers , J400) |> tbl_xts(cols_to_xts = J400, spread_by = Tickers)

Returns_xts <- T40_a |> select(date, Tickers , Return) |> tbl_xts(cols_to_xts = Return, spread_by = Tickers)

# Set NA's to null to use PA's Safe_returns.Portfolio command

Wghts_ALSI_xts[is.na(Wghts_ALSI_xts)] <- 0
Wghts_SWIX_xts[is.na(Wghts_SWIX_xts)] <- 0
Returns_xts[is.na(Returns_xts)] <- 0

# Now I calculate the weighed (uncapped) portfolio returns

Port_Ret_ALSI <- rmsfuns::Safe_Return.portfolio(R = Returns_xts, weights = Wghts_ALSI_xts, lag_weights = T) |> 
    
                 xts_tbl() |> rename(ALSI_Ret = portfolio.returns)

Port_Ret_SWIX <- rmsfuns::Safe_Return.portfolio(R = Returns_xts, weights = Wghts_SWIX_xts, lag_weights = T) |> 
    
                 xts_tbl() |> rename(SWIX_Ret = portfolio.returns)

# Now I combine the above two weighted portfolio returns

Merged_Port_Ret <- inner_join(Port_Ret_ALSI, Port_Ret_SWIX, by= "date")

# I verify that my by hand calc and the Safe_return calc is the same:
# df_Port_ret |> select(date, ALSI_pret, SWIX_pret) |> group_by(date, ALSI_pret, SWIX_pret ) |> summarise()
# Happy ---> They are the same

```

```{r, Portfolios cumulative returns}

# Now I proceed to calculate the Portfolios' cumulative return and plot it

Cum_ret <- Merged_Port_Ret |> arrange(date) |> 
    
    mutate(across(.cols = -date, .fns = ~cumprod(1 + .))) |> 
    
    mutate(across(.cols = -date, .fns = ~./first(.))) |> # Start at 1
    
    rename(ALSI = ALSI_Ret, SWIX= SWIX_Ret) |> 
    
    pivot_longer(cols=-date, names_to = "Index", values_to = "Cumret")
    
# And finally the plot

Indexes_Cum_ret_plot <-    Cum_ret |> 
       
       ggplot() + 
  
  geom_line(aes(date, Cumret , color = Index), size = 0.6, alpha = 0.7) +
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "%", caption = "Note:\nCalculation own",
       title = "Cumulative Returns of ALSI and SWIX Indexes",
       subtitle = "")

# Finplot for finishing touches:

fmxdat::finplot(Indexes_Cum_ret_plot, x.vert = T, x.date.type = "%Y", x.date.dist = "1 year", darkcol = T)

```

From the plot above, the cumulative weighted returns for the ALSI and SWIX indexes are strickingly similar, however, since the onset of COVID-19, the ALSI has achieved higher returns.

##  Weighted return contribition of each sector

I now create the final tbl that includes the weighted return contribution of each sector  to the overall weighted portfolio return (daily). From the plots produced below, the ALSI has a over time applied a larger weight to the Resources sector than the SWIX, where the SWIX applies relatively larger weights to Financials and Industrials. 

The weighted contribution per sector confirms that the additional weight the ALSI applies to the Resources sector compared to the SWIX is what generated its higher returns since the onset of COVID-19. 


```{r, Portfolio Returns subdivided into Market Caps Contributions}



df_Port_ret_final <- df_Port_ret |> arrange(date) |> 
    
    group_by(date, Sector) |>       # Group by Market Cap to calc each category's contr to wieghted portf return
    
    mutate(ALSI_wght_sector = coalesce(J400, 0)) |> # Make NA's 0 to use PA later
    
    mutate(SWIX_wght_sector = coalesce(J200, 0)) |>
    
    mutate(ALSI_wret = coalesce(ALSI_wret, 0)) |> 
    
    mutate(SWIX_wret = coalesce(SWIX_wret, 0)) |>
    
    mutate(ALSI_ret_sector = sum(ALSI_wret, na.rm = T)) |>  # The weight-contribution of each sector on each day
    
    mutate(SWIX_ret_sector = sum(SWIX_wret, na.rm = T)) |>

    mutate(ALSI_wght_sector = sum(J400, na.rm = T)) |>  # The weighted return contribution of each sector on each day
    
    mutate(SWIX_wght_sector = sum(J200, na.rm = T)) |>
    
    ungroup()

ALSI_wght_sector <-  df_Port_ret_final |> select(date, Sector ,ALSI_wght_sector) |> group_by(Sector) |> unique() |> 
    tbl_xts(cols_to_xts = ALSI_wght_sector, spread_by = Sector)

SWIX_wght_sector <- df_Port_ret_final |> select(date, Sector ,SWIX_wght_sector) |> group_by(Sector) |> unique() |> 
    tbl_xts(cols_to_xts = SWIX_wght_sector, spread_by = Sector)

ALSI_ret_sector <- df_Port_ret_final |> select(date, Sector ,ALSI_ret_sector) |> group_by(Sector) |> unique() |> 
    tbl_xts(cols_to_xts = ALSI_ret_sector, spread_by = Sector)

SWIX_ret_sector <- df_Port_ret_final |> select(date, Sector ,SWIX_ret_sector) |> group_by(Sector) |> unique() |> 
    tbl_xts(cols_to_xts = SWIX_ret_sector, spread_by = Sector)



 ALSI_RetPort_sector <- 
      rmsfuns::Safe_Return.portfolio(ALSI_ret_sector, 
                                     
                       weights = ALSI_wght_sector, lag_weights = TRUE,
                       
                       verbose = TRUE, contribution = TRUE, 
                       
                       value = 1000, geometric = TRUE) 

 SWIX_RetPort_sector <- 
      rmsfuns::Safe_Return.portfolio(SWIX_ret_sector, 
                                     
                       weights = SWIX_wght_sector, lag_weights = TRUE,
                       
                       verbose = TRUE, contribution = TRUE, 
                       
                       value = 1000, geometric = TRUE)
    

 ALSI_RetPort_sector$BOP.Weight  %>% .[endpoints(.,'months')] %>% chart.StackedBar()
 
  SWIX_RetPort_sector$BOP.Weight  %>% .[endpoints(.,'months')] %>% chart.StackedBar()

  ALSI_RetPort_sector$contribution |> chart.CumReturns(legend.loc = "bottom")
  
  SWIX_RetPort_sector$contribution |> chart.CumReturns(legend.loc = "bottom")
 
```

Pull the rebalance days, 

```{r, Rebalance days}

# I firt pull the effective rebalance dates

Rebalance_Days <-RebDays |> filter(Date_Type %in% c("Effective Date")) |> pull(date)
    
# And now for both Indexes I create a capped weights tbl for rebalancing purposes

rebalance_col_ALSI <- T40_a |> 
    
    filter(date %in% Rebalance_Days) |> 
    
    select(date, Tickers, J200) |> 
    
    rename(weight = J200) |> 
    
    mutate(RebalanceTime = format(date, "%Y_%b")) |> 
    
    mutate(weight= coalesce(weight, 0))
    
 rebalance_col_SWIX <- T40_a |> 
    
    filter(date %in% Rebalance_Days) |> 
    
    select(date, Tickers, J400) |> 
     
     rename(weight = J400) |> 
     
     mutate(RebalanceTime = format(date, "%Y_%b")) |> 
     
      mutate(weight= coalesce(weight, 0))
    
```


```{r, lets apply capping to the indexes}

Proportional_Cap_Foo <- function(df_Cons, W_Cap = 0.08){
  
  # Let's require a specific form from the user... Alerting when it does not adhere this form
  if( !"weight" %in% names(df_Cons)) stop("... for Calc capping to work, provide weight column called 'weight'")
  
  if( !"date" %in% names(df_Cons)) stop("... for Calc capping to work, provide date column called 'date'")
  
  if( !"Tickers" %in% names(df_Cons)) stop("... for Calc capping to work, provide id column called 'Tickers'")

  # First identify the cap breachers...
  Breachers <- 
    df_Cons %>% filter(weight > W_Cap) %>% pull(Tickers)
  
  # Now keep track of breachers, and add to it to ensure they remain at 10%:
  if(length(Breachers) > 0) {
    
    while( df_Cons %>% filter(weight > W_Cap) %>% nrow() > 0 ) {
      
      
      df_Cons <-
        
        bind_rows(
          
          df_Cons %>% filter(Tickers %in% Breachers) %>% mutate(weight = W_Cap),
          
          df_Cons %>% filter(!Tickers %in% Breachers) %>% 
            mutate(weight = (weight / sum(weight, na.rm=T)) * (1-length(Breachers)*W_Cap) )
          
        )
      
      Breachers <- c(Breachers, df_Cons %>% filter(weight > W_Cap) %>% pull(Tickers))
      
    }

    if( sum(df_Cons$weight, na.rm=T) > 1.001 | sum(df_Cons$weight, na.rm=T) < 0.999 | max(df_Cons$weight, na.rm = T) > W_Cap) {
      
      stop( glue::glue("For the Generic weight trimming function used: the weight trimming causes non unit 
      summation of weights for date: {unique(df_Cons$date)}...\n
      The restriction could be too low or some dates have extreme concentrations...") )
      
    }
    
  } else {
    
  }
  
  df_Cons
  
  }
  

# Now, to map this across all the dates, I purrr::map_df 
Capped_ALSI_10 <- 
    
    rebalance_col_ALSI |> 

    group_split(RebalanceTime) |> 
    
    map_df(~Proportional_Cap_Foo(., W_Cap = 0.1) ) |>  select(-RebalanceTime)
  
# Now I do the same for a 6% cap:

Capped_ALSI_6 <- 
    
    rebalance_col_ALSI |> 

    group_split(RebalanceTime) |> 
    
    map_df(~Proportional_Cap_Foo(., W_Cap = 0.06) ) |>  select(-RebalanceTime)

Capped_SWIX_10 <- 
    
    rebalance_col_ALSI |> 

    group_split(RebalanceTime) |> 
    
    map_df(~Proportional_Cap_Foo(., W_Cap = 0.1) ) |>  select(-RebalanceTime)
  
Capped_SWIX_6 <- 
    
    rebalance_col_ALSI |> 

    group_split(RebalanceTime) |> 
    
    map_df(~Proportional_Cap_Foo(., W_Cap = 0.06) ) |>  select(-RebalanceTime)

# # Testing if the max weight is correct for all 4 tbl above

Capped_ALSI_10 %>% pull(weight) %>% max(.) 
Capped_ALSI_6 %>% pull(weight) %>% max(.) 
Capped_SWIX_10 %>% pull(weight) %>% max(.) 
Capped_SWIX_6 %>% pull(weight) %>% max(.) # Success!!

```





## 6% and 10% Capped indexes weighted returns

I now analyse the impact different capping levels would have had on both the SWIX and ALSI (6% and 10%).

From the ALSI capped figure, a capping of 10% has virtually the same cumulative returns as when uncapped, with significantly lower returns when capped at 6%. This is likely due to this 6% capping most of the relatively higher returns generated from the Resources sector.

On the other hand, from the SWIX capped figure, the 10% cap would have improved its return since the onset of COVID, likely due to a reduction in weighted contribution from the weaker performing Industrial and Financial sectors. 


```{r, Newly capped index weighted returns}

####For ALSI capped at 10%#####

wghts_ALSI_10 <- 
  Capped_ALSI_10 %>% 
  tbl_xts(cols_to_xts = weight, spread_by = Tickers)

ret_ALSI_10 <- 
  T40_a %>% 
  
  filter(Tickers %in% unique(Capped_ALSI_10$Tickers) ) %>% 
  
  tbl_xts(cols_to_xts = Return, spread_by = Tickers)

wghts_ALSI_10[is.na(wghts_ALSI_10)] <- 0

ret_ALSI_10[is.na(ret_ALSI_10)] <- 0

ALSI_10_Idx <- 
  rmsfuns::Safe_Return.portfolio(R = ret_ALSI_10, weights = wghts_ALSI_10, lag_weights = T) |> 
  
  # Then I make it a tibble:
  xts_tbl() |>  
  
  rename(ALSI_10_Idx = portfolio.returns)

####For ALSI capped at 6%#####

wghts_ALSI_6 <- 
  Capped_ALSI_6 %>% 
  tbl_xts(cols_to_xts = weight, spread_by = Tickers)

ret_ALSI_6 <- 
  T40_a %>% 
  
  filter(Tickers %in% unique(Capped_ALSI_6$Tickers) ) %>% 
  
  tbl_xts(cols_to_xts = Return, spread_by = Tickers)

wghts_ALSI_6[is.na(wghts_ALSI_6)] <- 0

ret_ALSI_6[is.na(ret_ALSI_6)] <- 0

ALSI_6_Idx <- 
  rmsfuns::Safe_Return.portfolio(R = ret_ALSI_6, weights = wghts_ALSI_6, lag_weights = T) |> 
  
  # Then I make it a tibble:
  xts_tbl() |>  
  
  rename(ALSI_6_Idx = portfolio.returns)

####For SWIX capped at 10%#####

wghts_SWIX_10 <- 
  Capped_SWIX_10 %>% 
  tbl_xts(cols_to_xts = weight, spread_by = Tickers)

ret_SWIX_10 <- 
  T40_a %>% 
  
  filter(Tickers %in% unique(Capped_SWIX_10$Tickers) ) %>% 
  
  tbl_xts(cols_to_xts = Return, spread_by = Tickers)

wghts_SWIX_10[is.na(wghts_SWIX_10)] <- 0

ret_SWIX_10[is.na(ret_SWIX_10)] <- 0

SWIX_10_Idx <- 
  rmsfuns::Safe_Return.portfolio(R = ret_SWIX_10, weights = wghts_SWIX_10, lag_weights = T) |> 
  
  # Then I make it a tibble:
  xts_tbl() |>  
  
  rename(SWIX_10_Idx = portfolio.returns)



####For SWIX capped at 6%#####

wghts_SWIX_6 <- 
  Capped_SWIX_6 %>% 
  tbl_xts(cols_to_xts = weight, spread_by = Tickers)

ret_SWIX_6 <- 
  T40_a %>% 
  
  filter(Tickers %in% unique(Capped_SWIX_6$Tickers) ) %>% 
  
  tbl_xts(cols_to_xts = Return, spread_by = Tickers)

wghts_SWIX_6[is.na(wghts_SWIX_6)] <- 0

ret_SWIX_6[is.na(ret_SWIX_6)] <- 0

SWIX_6_Idx <- 
  rmsfuns::Safe_Return.portfolio(R = ret_SWIX_6, weights = wghts_SWIX_6, lag_weights = T) |> 
  
  # Then I make it a tibble:
  xts_tbl() |>  
  
  rename(SWIX_6_Idx = portfolio.returns)



```


```{r, Plot the capped indexes}

Capped_df_final <- ALSI_6_Idx |> inner_join(ALSI_10_Idx, by ="date") |> 
    inner_join(SWIX_6_Idx, by ="date") |> 
    inner_join(SWIX_10_Idx, by ="date") |>
    
    arrange(date) |> 
    mutate(across(.cols = -date, .fns = ~cumprod(1+.))) |> # cumulative returns
    mutate(across(.cols = -date, .fns = ~./first(.))) |>   # Start at 1
    inner_join(Cum_ret |> pivot_wider(names_from= "Index", values_from= "Cumret"), by = "date") |> # Add the uncapped cumret 
    
    rename("ALSI Capped 6%" = ALSI_6_Idx, "ALSI Capped 10%" = ALSI_10_Idx,"SWIX Capped 6%" =  SWIX_6_Idx, "SWIX Capped 10%" =  SWIX_10_Idx, "Uncapped SWIX" = SWIX, "Uncapped ALSI" = ALSI) |> # rename for clarity
    
    pivot_longer(cols = -date, names_to = "Description", values_to = "Values")

# And now, at last, for the plot

capping_plot_ALSI <- 

Capped_df_final |> filter(Description %in% c("ALSI Capped 6%", "ALSI Capped 10%", "Uncapped ALSI"))  |> 

    ggplot() + 
  
  geom_line(aes(date, Values , color = Description), size = 0.8, alpha = 0.7) +
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "%", caption = "Note:\nCalculation own",
       title = "Cumulative Returns of ALSI Capped at 6% and 10%",
       subtitle = "")

capping_plot_SWIX <- 

Capped_df_final |> filter(Description %in% c("SWIX Capped 6%", "SWIX Capped 10%", "Uncapped SWIX"))  |> 

    ggplot() + 
  
  geom_line(aes(date, Values , color = Description), size = 0.8, alpha = 0.7) +
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "%", caption = "Note:\nCalculation own",
       title = "Cumulative Returns of SWIX Capped at 6% and 10%",
       subtitle = "")

# Finplot for finishing touches:

fmxdat::finplot(capping_plot_ALSI, x.vert = T, x.date.type = "%Y", x.date.dist = "1 year", darkcol = T)

fmxdat::finplot(capping_plot_SWIX, x.vert = T, x.date.type = "%Y", x.date.dist = "1 year", darkcol = T)


```





# Question 4

For this question I using the Top 40 Index data from question 3 to calculate the concentration of returns among the
ALSI constituents (J200) by considering it from a Principal Component Analysis (PCA) perspective. 

I plot the Scree plot (percentage of explained variances) and the Cos2 plot for the constituants that explain most of the volatility (Quality of representation). 

Note that; A high cos2 indicates a good representation of the variable on the principal component. Whereas a  low cos2 indicates that the variable is not perfectly represented by the PCs.

## Scree plot

```{r}

pacman::p_load("RiskPortfolios", "FactoMineR", "factoextra", "broom")

#  Extract the weighted returns for each constituent from Q3, and change to wide format to use princomp.
# In addition, I transform the simple returns to log returns and mean-centre them


PCA_data <- df_Port_ret |> select(date, Tickers, ALSI_wret) |>
    
    mutate(log_ret = exp(ALSI_wret)-1) |>            # convert to log returns
    
    arrange(date) |> group_by(Tickers) |> 
    
    mutate(log_ret = log_ret - mean(log_ret)) |>   # Mean-centering
    
    ungroup() |> select(-ALSI_wret) |> 
    
    pivot_wider(names_from = "Tickers", values_from = "log_ret")

# Now I source the following function from the tut:

impute_missing_returns <- function(return_mat, impute_returns_method = "NONE", Seed = 1234){
  # Make sure we have a date column called date:
  if( !"date" %in% colnames(return_mat) ) stop("No 'date' column provided in return_mat. Try again please.")

  # Note my use of 'any' below...
  # Also note that I 'return' return_mat - which stops the function and returns return_mat. 
  if( impute_returns_method %in% c("NONE", "None", "none") ) {
    if( any(is.na(return_mat)) ) warning("There are missing values in the return matrix.. Consider maybe using impute_returns_method = 'Drawn_Distribution_Own' / 'Drawn_Distribution_Collective'")
    return(return_mat)
  }

  
  if( impute_returns_method  == "Average") {

    return_mat <-
      return_mat %>% gather(Stocks, Returns, -date) %>%
      group_by(date) %>%
      mutate(Avg = mean(Returns, na.rm=T)) %>%
      mutate(Avg = coalesce(Avg, 0)) %>% # date with no returns - set avg to zero
      ungroup() %>%
      mutate(Returns = coalesce(Returns, Avg)) %>% select(-Avg) %>% spread(Stocks, Returns)

    # That is just so much easier when tidy right? See how I gathered and spread again to give back a wide df?
    
  } else

    if( impute_returns_method  == "Drawn_Distribution_Own") {

      set.seed(Seed)
      N <- nrow(return_mat)
      return_mat <-

        left_join(return_mat %>% gather(Stocks, Returns, -date),
                  return_mat %>% gather(Stocks, Returns, -date) %>% group_by(Stocks) %>%
                    do(Dens = density(.$Returns, na.rm=T)) %>%
                    ungroup() %>% group_by(Stocks) %>% # done to avoid warning.
                    do(Random_Draws = sample(.$Dens[[1]]$x, N, replace = TRUE, prob=.$Dens[[1]]$y)),
                  by = "Stocks"
        ) %>%  group_by(Stocks) %>% mutate(Row = row_number()) %>% mutate(Returns = coalesce(Returns, Random_Draws[[1]][Row])) %>%
        select(-Random_Draws, -Row) %>% ungroup() %>% spread(Stocks, Returns)

    } else

      if( impute_returns_method  == "Drawn_Distribution_Collective") {

        set.seed(Seed)
        NAll <- nrow(return_mat %>% gather(Stocks, Returns, -date))

        return_mat <-
          bind_cols(
          return_mat %>% gather(Stocks, Returns, -date),
          return_mat %>% gather(Stocks, Returns, -date) %>%
            do(Dens = density(.$Returns, na.rm=T)) %>%
            do(Random_Draws = sample(.$Dens[[1]]$x, NAll, replace = TRUE, prob=.$Dens[[1]]$y)) %>% unnest(Random_Draws)
          ) %>%
          mutate(Returns = coalesce(Returns, Random_Draws)) %>% select(-Random_Draws) %>% spread(Stocks, Returns)

      } else

        if( impute_returns_method  == "Zero") {
        warning("This is probably not the best idea but who am I to judge....")
          return_mat[is.na(return_mat)] <- 0

        } else
          stop("Please provide a valid impute_returns_method method. Options include:\n'Average', 'Drawn_Distribution_Own', 'Drawn_Distribution_Collective' and 'Zero'.")
}


options(scipen=999) # To remove scientific notation

return_matrix <- 
  impute_missing_returns(PCA_data, impute_returns_method = "Drawn_Distribution_Collective", Seed = as.numeric(format( Sys.time(), "%Y%d%H%M")))

# Remove the date column
return_matrix_Nodate <- data.matrix(return_matrix[, -1])

# Simple Sample covariance and mean:

Sigma <- RiskPortfolios::covEstimation(return_matrix_Nodate)
Mu <- RiskPortfolios::meanEstimation(return_matrix_Nodate)

    
# Conduct PCA:
    
pca <- prcomp(return_matrix_Nodate)   
    
# And now for plotting
    
```


```{r, scree plot}

fviz_screeplot(pca, ncp=10)

```

The plot above shows: nearly 20% of variation in the ALSI index is explained by a single component. It does not give insight into what this component or factor might be, but it tells us how this component is calculated linearly. In addition, excluding the large portion explained by the first two PC's, the remaining seven PC's accounts for roughly the same amount of variation in the ALSI. 

## Cos2 plot 

Given the large contribution of the first two components, I generate the cos2 plot below for the first two PC's. NPN, BHP, and AGL have high cos2 values, indicating a good representation of the variable on the first two PC's. 

In conclusion; NPN, BHP, and AGL constituents explain a significant portion of the variation in the ALSI Top 40 index. 


```{r, cos2 plot}

fviz_cos2(pca, choice = "var", axes = 1:2, top = 10)

```



# Question 5

The purpose of this question is twofold; To comment on the following two statements:

* The South African rand (ZAR) has over the past few years been one of the most volatile
currencies;
* The ZAR has generally performed well during periods where G10 currency carry trades
have been favourable and these currency valuations relatively cheap. Globally, it has been
one of the currencies that most benefit during periods where the Dollar is comparatively
strong, indicating a risk-on sentiment.

## Loading data

```{r, loading data}

cncy <- read_rds("data/currencies.rds")
cncy_Carry <- read_rds("data/cncy_Carry.rds")
cncy_value <- read_rds("data/cncy_value.rds")
cncyIV <- read_rds("data/cncyIV.rds")
bbdxy <- read_rds("data/bbdxy.rds")

Currency_names_full <- cncy |> summarise(Name) |> unique() |> pull() 


pacman::p_load("MTS", "robustbase","fGarch")
pacman::p_load("tidyverse", "devtools", "rugarch", "rmgarch", 
    "forecast", "tbl2xts", "lubridate", "PerformanceAnalytics", 
    "ggthemes", "MTS")

print(Currency_names_full)

```


## Sample SD comparison of selected currencies

Lets investigate the first point to comment on : "The South African rand (ZAR) has over the past few years been one of the most volatile currencies". To investigate this, I select 8 countries' currencies: Brazil, EU, India, SA, Turkey, Poland, Zambia, and the UK (The justification of these choices will be discussed in the final report), and compare their respective dollar exchange rates. 


```{r}

# Remove the common second part of names

Curr_df <- cncy |> mutate(Name = gsub( "_Cncy", "", Name)) |> mutate(Name = gsub( "_Inv", "", Name)) 



Countries_to_consider <- c("Brazil","EU","India","Poland","Zambia","Turkey","SouthAfrica","UK")

# filter countries to consider

Curr_df <- Curr_df |> filter(Name %in% Countries_to_consider) 


```

To get an initial comparison of volatility of our selected currencies, I plot the sample standard deviation in log growth against the dollar. I also filter the dates to consider the period from 2006 onwards to remove extremely volatile periods for international currencies. 

```{r}


SD_plot_df <- Curr_df |>  arrange(date) |> 
    
    group_by(Name) |> 
    
    mutate(Growth = log(Price) - lag(log(Price))) |> 
    
    filter(date > dplyr::first(date)) |>  
    
    mutate(scaledgrowth = Growth - mean(Growth, rm.na = T)) |>     # Scale the Growth by demeaning
    
    mutate(SampleSD = (sqrt(scaledgrowth^2))) |> 
    
    ungroup() |> 
    
    filter(date > lubridate::ymd(20041231))




Scaledgrowth_plot_df <-  SD_plot_df |> 
 
    ggplot() + 
  
  geom_line(aes(date, scaledgrowth , color = Name), size = 0.6, alpha = 0.7) +
    
    facet_wrap(~Name, scales = "free_y")+
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "%", caption = "Note:\nCalculation own",
       title = "Scaled (demeaned) Log Growth of Respective Currencies to USD since 2005.",
       subtitle = "")

SD_plot <- SD_plot_df |>  

  ggplot() + 
  
   geom_line(aes(date, SampleSD , color = Name), size = 0.6, alpha = 0.7) +
    
    facet_wrap(~Name, scales = "free_y")+
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "%", caption = "Note:\nCalculation own",
       title = "Sample Standard Deviation of Respective Currencies to USD since 2005.",
       subtitle = "")

# Finplot for finishing touches:



fmxdat::finplot(Scaledgrowth_plot_df, x.vert = T, x.date.type = "%Y", x.date.dist = "2 years", darkcol = T)
fmxdat::finplot(SD_plot, x.vert = T, x.date.type = "%Y", x.date.dist = "2 years", darkcol = T)


```

From this initial inspection of sample SD, the plot indicates that the ZAR has recently not been the most volatile WITH RESPECT TO THE USD. However, lets take a deeper dive into the volatility of these currencies and try and fit a multivariate GARCH model, but by also including the Bloomberg Dollar Spot Index (BBDXY) as a variable; it tracks the performance of a basket of 10 leading global currencies versus the U.S. Dollar. It has a dynamically updated composition and represents a diverse set of currencies that are important from trade and liquidity perspectives.

Importantly, since the UK and a large part of the EU is almost always included in the BBDXY as the top 10 leading currencies, I remove them from the model as their effect against the dollar is endogenous. From this multivariate GARCH, I can estimate the conditional time-varying correlation the ZAR has versus the other currencies, as well as see whether the ZAR is one of the currencies that benefits the most during periods where the Dollar is comparatively strong, indicating a risk-on sentiment.

## MV Conditional Heteroskedasticity tests

I conduct MV Portmanteau tests using MarchTest from MTS package, after I join the BBDXY data and drop the UK and EU as discussed above. 

```{r}

# Calculate the scaled (demeaned) log growth for BBDXY 

ln_bbdxy <- bbdxy |> arrange(date) |> 
    
    mutate(Growth = log(Price) - lag(log(Price))) |> 
    
    filter(date > dplyr::first(date)) |>  
    
    mutate(scaledgrowth = Growth - mean(Growth, rm.na = T)) |>     # Scale the Growth by demeaning
    
    mutate(SampleSD = (sqrt(scaledgrowth^2))) |> 
    
    ungroup() |> 
    
    filter(date > lubridate::ymd(20041231))

# Now I merge it with the rest, and remove the UK, EU, and Zambia

gwt <- SD_plot_df |> select(date, Name, Growth) |> filter(!(Name %in% c("EU", "UK", "Zambia"))) |> 
    
    pivot_wider(names_from = "Name", values_from = "Growth") |> 

    left_join(ln_bbdxy |> select(date, Growth) |> rename(BBDXY = Growth), by= c("date")) |> 
    
    pivot_longer(cols = -date, values_to = "Growth", names_to = "Currency") |> 
    
    mutate(Growth = coalesce(Growth, 0)) 

# Change to xts format

gwt_xts <- gwt |> 
    
    tbl_xts(cols_to_xts = Growth, spread_by = Currency)

# MV Portmanteau tests

MarchTest(gwt_xts)

```

The MARCH test indicates that all the MV portmanteau tests reject the null of no conditional heteroskedasticity, motivating our use of MVGARCH models.

## DCC MV-GARCH MODEL

I decide to use a DCC MVGARCH Model; DCC models offer a simple and more parsimonious means of doing MV-vol modelling. In particular, it relaxes the constraint of a fixed correlation structure (assumed by the CCC model), to allow for estimates of time-varying correlation. 

```{r, DCC MVGARCH}

# As in the tut, I select a VAR order of zero for the mean equation, and simply use the mean of each series.
# The mean equation is thus in our case simply: Growth = mean(Growth) + et

# Then, for every series, a standard univariate GARCH(1,1) is run - giving us:
# et and sigmat, which is then used to calculate the standardized resids, zt, which is used in DCC calcs after.

DCCPre <- dccPre(gwt_xts, include.mean = F, p=0) # Find a nice way to put this in a table

```



```{r}

Vol <- DCCPre$marVol

colnames(Vol) <- colnames(gwt_xts)

Vol <- 
  data.frame( cbind( date = index(gwt_xts), Vol)) |>  # Add date column which dropped away...
  mutate(date = as.Date(date)) |>  tibble::as_tibble()  # make date column a date column...



TidyVol <- Vol |>  pivot_longer(names_to = "Stocks", values_to =  "Sigma", cols =  -date)

TidyVol_plot <- TidyVol |> ggplot() + 
  
  geom_line(aes(date, Sigma , color = Stocks), size = 0.9, alpha = 0.6) +
    
  
   fmxdat::theme_fmx(title.size = fmxdat::ggpts(30), 
                    subtitle.size = fmxdat::ggpts(0),
                    caption.size = fmxdat::ggpts(25),
                    CustomCaption = T) + 
    
  fmxdat::fmx_cols() + 
  
  labs(x = "", y = "Sigma", caption = "Note:\nCalculation own",
       title = "DCC GARCH: Estimated Volatility (Sigma) for Each Currency",
       subtitle = "Notice this includes the Bloomberg Dollar Spot Index (BBDXY)")
    
# And finally touches with finplot    

fmxdat::finplot(TidyVol_plot, x.vert = T, x.date.type = "%Y", x.date.dist = "2 years", darkcol = F)


```

The volatility estimates are slightly different than the simple sample SD graph above, in that the ZAR's volatility has increased relative to the other currencies, however, Brazil and Turkey still showcases more volatility, even in the recent few years. 

## Calculating the DCC Model

I will now use the standardized residuals to calculate the DCC Model


```{r}
# The standardized residuals

StdRes <- DCCPre$sresi

# I first do the detach trick from the tut:

pacman::p_load(tidyverse,fmxdat, rmsfuns, tbl2xts, tidyr, ggpubr, broom,rstatix, modelr )

detach("package:tidyverse", unload=TRUE)
detach("package:fmxdat", unload=TRUE)
detach("package:rmsfuns", unload=TRUE)
detach("package:tbl2xts", unload=TRUE)
detach("package:ggpubr", unload=TRUE)
detach("package:rstatix", unload=TRUE)
detach("package:modelr", unload=TRUE)
detach("package:broom", unload=TRUE)
detach("package:tidyr", unload=TRUE)
detach("package:dplyr", unload=TRUE)

DCC <- dccFit(StdRes,type = "Engle") 

pacman::p_load(tidyverse,fmxdat, rmsfuns, tbl2xts, tidyr, ggpubr, broom,rstatix, modelr )

```



```{r}


Rhot <- DCC$rho.t
# Right, so it gives us all the columns together in the form:
# X1,X1 ; X1,X2 ; X1,X3 ; ....

# So, let's be clever about defining more informative col names. 
# I will create a renaming function below:
ReturnSeries = gwt_xts
DCC.TV.Cor = Rhot

renamingdcc <- function(ReturnSeries, DCC.TV.Cor) {
  
ncolrtn <- ncol(ReturnSeries)
namesrtn <- colnames(ReturnSeries)
paste(namesrtn, collapse = "_")

nam <- c()
xx <- mapply(rep, times = ncolrtn:1, x = namesrtn)
# Now let's be creative in designing a nested for loop to save the names corresponding to the columns of interest.. 

# TIP: draw what you want to achieve on a paper first. Then apply code.

# See if you can do this on your own first.. Then check vs my solution:

nam <- c()
for (j in 1:(ncolrtn)) {
for (i in 1:(ncolrtn)) {
  nam[(i + (j-1)*(ncolrtn))] <- paste(xx[[j]][1], xx[[i]][1], sep="_")
}
}

colnames(DCC.TV.Cor) <- nam

# So to plot all the time-varying correlations wrt SBK:
 # First append the date column that has (again) been removed...
DCC.TV.Cor <- 
    data.frame( cbind( date = index(ReturnSeries), DCC.TV.Cor)) %>% # Add date column which dropped away...
    mutate(date = as.Date(date)) %>%  tbl_df() 

DCC.TV.Cor <- DCC.TV.Cor %>% gather(Pairs, Rho, -date)

DCC.TV.Cor

}

# Let's see if our function works! Excitement!
Rhot <- 
  renamingdcc(ReturnSeries = gwt_xts, DCC.TV.Cor = Rhot)

head(Rhot %>% arrange(date))


```

And now I create a plot the ZAR relative to the other currencies

```{r}
# Let's now create a plot for all the stocks relative to the other stocks...
g1 <- 
  ggplot(Rhot |>  filter(grepl("SouthAfrica_", Pairs ), !grepl("_SouthAfrica", Pairs)) ) + 
  geom_line(aes(x = date, y = Rho, colour = Pairs)) + 
  theme_hc() +
  ggtitle("Dynamic Conditional Correlations: South Africa (ZAR)")

print(g1)
```

For a clearer picture, lets only plot the SouthAfrica_BBDXY plot

```{r}
g2 <- 
  ggplot(Rhot |>  filter(Pairs %in% c("SouthAfrica_BBDXY"))) + 
  geom_line(aes(x = date, y = Rho, colour = Pairs)) + 
  theme_hc() +
  ggtitle("Dynamic Conditional Correlation: South Africa (ZAR) and BBDXY")

print(g2)


```

